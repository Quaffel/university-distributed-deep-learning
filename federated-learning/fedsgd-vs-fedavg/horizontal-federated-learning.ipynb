{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lab tutorial presents the findings and uses part of the experimental methodology from the [original Federated Learning](https://arxiv.org/pdf/1602.05629.pdf) paper. In horizontal federated learning, all clients have access to the same complete model architecture, which they train on local data, sharing information about model updates but not their data.\n",
    "\n",
    "Before starting, make sure to follow the overall setup for the labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://blogs.nvidia.com/blog/what-is-federated-learning/\" target=\"_blank\">\n",
    "    <img src=\"https://blogs.nvidia.com/wp-content/uploads/2019/10/federated_learning_animation_still_white.png\" alt=\"FL Visualization\" style=\"width:50%;\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything else, we download, load, and preprocess the [MNIST dataset](https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits), which we will use for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using accelerator 'mps'\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_path = \"./data\"\n",
    "ETA = \"\\N{GREEK SMALL LETTER ETA}\"\n",
    "\n",
    "if torch.accelerator.is_available():\n",
    "    device = torch.accelerator.current_accelerator()\n",
    "    print(f\"Using accelerator '{device}'\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARN: No accelerator found, running on CPU\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # normalize by mean and standard devia,\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST(data_path, train=False, download=False, transform=transform),\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    batch_size=10000,\n",
    "    generator=torch.Generator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a small convolutional neural network that will serve as our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MnistCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistCnn, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Log softmax rather than softmax due to negative log likelihood loss.\n",
    "        # log_softmax rather than two separate operations for numerical stability\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can define a helper method, which, given a model, a loader for iterating through a set of data, and an optimizer for updating the model trains one epoch (i.e., learns going through all the available data once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "# negative log likelihood loss\n",
    "loss_function = F.nll_loss\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: torch.nn.Module, loader: DataLoader, optimizer: Optimizer\n",
    ") -> None:\n",
    "    model.train()\n",
    "\n",
    "    for batch_features, batch_target in loader:\n",
    "        batch_features = typing.cast(torch.Tensor, batch_features).to(device)\n",
    "        batch_target = typing.cast(torch.Tensor, batch_target).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = model(batch_features)\n",
    "\n",
    "        batch_loss = loss_function(batch_output, batch_target)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define another utility method that splits the dataset into several chunks.\n",
    "\n",
    "We assign samples within chunks in an IID (independent and identically distributed) fashion or allow only two labels to exist in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_distinct_values(targets: typing.Iterable[typing.Any]):\n",
    "    targets_count = Counter(targets)\n",
    "\n",
    "    for target, count in targets_count.items():\n",
    "        print(f\"Target {target}: {count} records\")\n",
    "\n",
    "    print(f\"Total: {targets_count.total()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 5: 5421 records\n",
      "Target 0: 5923 records\n",
      "Target 4: 5842 records\n",
      "Target 1: 6742 records\n",
      "Target 9: 5949 records\n",
      "Target 2: 5958 records\n",
      "Target 3: 6131 records\n",
      "Target 6: 5918 records\n",
      "Target 7: 6265 records\n",
      "Target 8: 5851 records\n",
      "Total: 60000\n"
     ]
    }
   ],
   "source": [
    "count_distinct_values(train_dataset.targets.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "type IndexVector = np.ndarray[tuple[int], np.dtype[np.long]]\n",
    "\n",
    "\n",
    "def _get_rng(generator_or_seed: npr.Generator | int) -> npr.Generator:\n",
    "    if type(generator_or_seed) == int:\n",
    "        return npr.default_rng(generator_or_seed)\n",
    "    else:\n",
    "        return typing.cast(npr.Generator, generator_or_seed)\n",
    "\n",
    "\n",
    "def index_uniformly(\n",
    "    partitions_count: int, generator_or_seed: npr.Generator | int\n",
    ") -> list[IndexVector]:\n",
    "    generator = _get_rng(generator_or_seed)\n",
    "\n",
    "    shuffled_indices: IndexVector = generator.permutation(len(train_dataset))\n",
    "    return np.array_split(shuffled_indices, partitions_count)\n",
    "\n",
    "\n",
    "def _combine_partitions(\n",
    "    mini_partitions: list[IndexVector],\n",
    "    *,\n",
    "    mini_partitions_per_partition: int,\n",
    "    generator: npr.Generator,\n",
    ") -> list[IndexVector]:\n",
    "    if len(mini_partitions) % mini_partitions_per_partition != 0:\n",
    "        raise ValueError(\n",
    "            f\"expected to have exactly {mini_partitions_per_partition} mini-partitions per partition,\"\n",
    "            f\"got {len(mini_partitions)} mini-partitions\"\n",
    "        )\n",
    "\n",
    "    partitions_count = len(mini_partitions) // mini_partitions_per_partition\n",
    "    shuffled_partition_indices = generator.permutation(len(mini_partitions))\n",
    "\n",
    "    return [\n",
    "        np.concatenate(\n",
    "            [mini_partitions[partition_idx] for partition_idx in mini_partition_indices]\n",
    "        )\n",
    "        for mini_partition_indices in shuffled_partition_indices.reshape(\n",
    "            partitions_count, mini_partitions_per_partition\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def index_by_approximate_binary_target_partitions(\n",
    "    partitions_count: int, generator_or_seed: npr.Generator | int\n",
    ") -> list[IndexVector]:\n",
    "    generator = _get_rng(generator_or_seed)\n",
    "\n",
    "    targets = train_dataset.targets.numpy().copy()\n",
    "    generator.shuffle(targets)\n",
    "\n",
    "    sorted_indices: IndexVector = np.argsort(train_dataset.targets)\n",
    "    sorted_indices_partitions: list[IndexVector] = np.array_split(\n",
    "        sorted_indices, 2 * partitions_count\n",
    "    )\n",
    "\n",
    "    return _combine_partitions(\n",
    "        sorted_indices_partitions, mini_partitions_per_partition=2, generator=generator\n",
    "    )\n",
    "\n",
    "\n",
    "def index_by_binary_target_partitions(\n",
    "    partitions_count: int, generator_or_seed: npr.Generator | int\n",
    ") -> list[IndexVector]:\n",
    "    generator = _get_rng(generator_or_seed)\n",
    "\n",
    "    targets = train_dataset.targets.numpy()\n",
    "    generator.shuffle(targets)\n",
    "\n",
    "    client_indices = []\n",
    "    unique_targets = np.unique(targets)\n",
    "\n",
    "    unique_targets_count = unique_targets.shape[0]\n",
    "    mini_partitions_per_label = partitions_count // unique_targets_count\n",
    "    if partitions_count % unique_targets_count != 0:\n",
    "        raise ValueError(\n",
    "            \"expected number of partitions to be a multiple of the number of unique Ïtargets, \"\n",
    "            f\"got {partitions_count} partitions and {unique_targets_count} unique targets\"\n",
    "        )\n",
    "\n",
    "    for target in unique_targets:\n",
    "        label_indices = np.where(targets == target)[0]\n",
    "        label_shards = np.array_split(label_indices, mini_partitions_per_label)\n",
    "        client_indices.extend(label_shards)\n",
    "\n",
    "    return _combine_partitions(\n",
    "        client_indices, mini_partitions_per_partition=2, generator=generator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(\n",
    "    dataset: Dataset, partitions: list[IndexVector]\n",
    ") -> list[Subset[typing.Any]]:\n",
    "    return [\n",
    "        Subset(dataset, typing.cast(typing.Sequence[int], partition))\n",
    "        for partition in partitions\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== subset #0 ====\n",
      "Target 2: 3000 records\n",
      "Target 7: 3000 records\n",
      "Total: 6000\n",
      "==== subset #1 ====\n",
      "Target 3: 3000 records\n",
      "Target 6: 2935 records\n",
      "Target 7: 65 records\n",
      "Total: 6000\n",
      "==== subset #2 ====\n",
      "Target 1: 3665 records\n",
      "Target 2: 2335 records\n",
      "Total: 6000\n",
      "==== subset #3 ====\n",
      "Target 2: 623 records\n",
      "Target 3: 2377 records\n",
      "Target 5: 17 records\n",
      "Target 6: 2983 records\n",
      "Total: 6000\n",
      "==== subset #4 ====\n",
      "Target 3: 754 records\n",
      "Target 4: 2246 records\n",
      "Target 0: 2923 records\n",
      "Target 1: 77 records\n",
      "Total: 6000\n",
      "==== subset #5 ====\n",
      "Target 8: 51 records\n",
      "Target 9: 2949 records\n",
      "Target 0: 3000 records\n",
      "Total: 6000\n",
      "==== subset #6 ====\n",
      "Target 4: 596 records\n",
      "Target 5: 2404 records\n",
      "Target 7: 200 records\n",
      "Target 8: 2800 records\n",
      "Total: 6000\n",
      "==== subset #7 ====\n",
      "Target 5: 3000 records\n",
      "Target 4: 3000 records\n",
      "Total: 6000\n",
      "==== subset #8 ====\n",
      "Target 7: 3000 records\n",
      "Target 8: 3000 records\n",
      "Total: 6000\n",
      "==== subset #9 ====\n",
      "Target 9: 3000 records\n",
      "Target 1: 3000 records\n",
      "Total: 6000\n"
     ]
    }
   ],
   "source": [
    "sample_index = index_by_approximate_binary_target_partitions(10, 42)\n",
    "subsets = partition_dataset(train_dataset, sample_index)\n",
    "# \n",
    "for subset_idx, subset in enumerate(subsets):\n",
    "    print(f\"==== subset #{subset_idx} ====\")\n",
    "    subset_targets = list(int(it) for _, it in DataLoader(subset))\n",
    "    count_distinct_values(subset_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a short class for holding the results of training runs and the parameters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RoundParameters:\n",
    "    clients_count: int\n",
    "    active_clients_fraction: float\n",
    "    batch_size: int\n",
    "    local_epochs_count: int\n",
    "    learning_rate: float\n",
    "    seed: int\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RunMetrics:\n",
    "    wall_time: list[float] = dataclasses.field(default_factory=list)\n",
    "    message_count: list[int] = dataclasses.field(default_factory=list)\n",
    "    test_accuracy: list[float] = dataclasses.field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class RunResult:\n",
    "    algorithm: str\n",
    "    parameters: RoundParameters\n",
    "    metrics: RunMetrics = dataclasses.field(default_factory=RunMetrics)\n",
    "\n",
    "    def as_df(self) -> DataFrame:\n",
    "        table_data = {\n",
    "            \"round\": range(1, len(self.metrics.wall_time) + 1),\n",
    "            \"algorithm\": self.algorithm,\n",
    "            **dataclasses.asdict(self.parameters),\n",
    "            **dataclasses.asdict(self.metrics),\n",
    "        }\n",
    "\n",
    "        if table_data[\"batch_size\"] == -1:\n",
    "            table_data[\"batch_size\"] = \"\\N{INFINITY}\"\n",
    "\n",
    "        df = DataFrame(table_data)\n",
    "        df = df.rename(\n",
    "            columns={\"learning_rate\": ETA, \"message_count\": \"message_count (sum)\"}\n",
    "        )\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an abstract class as a template for all distributed learning clients, defining a method for outputting an update after training a given model on local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "\n",
    "class Client(typing.Protocol):\n",
    "    def update(self, weights: list[torch.Tensor], seed: int) -> list[torch.Tensor]:\n",
    "        ...\n",
    "\n",
    "\n",
    "class AbstractClient(ABC, Client):\n",
    "    def __init__(self, client_data: Subset, batch_size: int) -> None:\n",
    "        self.model = MnistCnn().to(device)\n",
    "        self.generator = torch.Generator()\n",
    "        self.loader_train = DataLoader(\n",
    "            client_data, batch_size=batch_size, shuffle=True,\n",
    "            drop_last=False, generator=self.generator)\n",
    "        \n",
    "    def build_local_model(self, weights: list[torch.Tensor]) -> torch.nn.Module:\n",
    "        model = MnistCnn().to(device)\n",
    "        with torch.no_grad():\n",
    "            for client_parameter, server_parameter_values in zip(\n",
    "                model.parameters(), weights\n",
    "            ):\n",
    "                client_parameter[:] = server_parameter_values\n",
    "                client_parameter.grad = None\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the flip side, a server needs to be able to run the (distributed) training process for a given number of rounds and test the current model it possesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(typing.Protocol):\n",
    "    def run(self, rounds: int) -> RunResult: ...\n",
    "\n",
    "\n",
    "class AbstractServer(ABC, Server):\n",
    "    def __init__(self, parameters: RoundParameters) -> None:\n",
    "        torch.manual_seed(parameters.seed)\n",
    "        self.parameters = parameters\n",
    "        self.model = MnistCnn().to(device)\n",
    "\n",
    "    def evaluate_accuracy(self) -> float:\n",
    "        self.model.eval()\n",
    "\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in test_loader:\n",
    "                batch_features = typing.cast(torch.Tensor, batch_features).to(device)\n",
    "                batch_targets = typing.cast(torch.Tensor, batch_targets).to(device)\n",
    "\n",
    "                batch_output: torch.Tensor = self.model(batch_features)\n",
    "\n",
    "                # index of output neuron/logit corresponds to label\n",
    "                batch_predictions = batch_output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "                correct_predictions += batch_predictions.eq(batch_targets.view_as(batch_predictions)).sum().item()\n",
    "                total_predictions += batch_predictions.size(dim=0)\n",
    "\n",
    "        print(\"correct: \", correct_predictions, \"total: \", total_predictions)\n",
    "        return correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the previously defined server template, we can even formulate a centralized variant, which does not involve clients, as a precursor to distributed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CentralizedServer(AbstractServer):\n",
    "    def __init__(self, learning_rate: float, batch_size: int, seed: int) -> None:\n",
    "        super().__init__(\n",
    "            RoundParameters(\n",
    "                clients_count=1,\n",
    "                active_clients_fraction=float(\"nan\"),\n",
    "                batch_size=batch_size,\n",
    "                local_epochs_count=1,\n",
    "                learning_rate=learning_rate,\n",
    "                seed=seed,\n",
    "            )\n",
    "        )\n",
    "        self.optimizer = SGD(params=self.model.parameters(), lr=learning_rate)\n",
    "        self.generator = torch.Generator()\n",
    "        self.loader_train = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            generator=self.generator,\n",
    "        )\n",
    "\n",
    "    def run(self, rounds: int) -> RunResult:\n",
    "        metrics = RunMetrics()\n",
    "\n",
    "        for epoch in tqdm(range(rounds), \"epoch\", leave=False):\n",
    "            self.generator.manual_seed(self.parameters.seed + epoch + 1)\n",
    "\n",
    "            wall_time_start = time.perf_counter()\n",
    "            train_epoch(self.model, self.loader_train, self.optimizer)\n",
    "            wall_time_end = time.perf_counter()\n",
    "\n",
    "            accuracy = self.evaluate_accuracy()\n",
    "            execution_time = wall_time_end - wall_time_start\n",
    "\n",
    "            metrics.test_accuracy.append(accuracy)\n",
    "            metrics.wall_time.append(execution_time)\n",
    "            metrics.message_count.append(-1)\n",
    "\n",
    "        return RunResult(\n",
    "            \"centralized\",\n",
    "            self.parameters,\n",
    "            metrics\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 1/5 [00:07<00:30,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  8131 total:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 2/5 [00:14<00:21,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9645 total:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 3/5 [00:21<00:14,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9786 total:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 4/5 [00:28<00:06,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9745 total:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9802 total:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "centralized_server = CentralizedServer(learning_rate=0.5, batch_size=1024, seed=42)\n",
    "result_centralized = centralized_server.run(rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>clients_count</th>\n",
       "      <th>active_clients_fraction</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>local_epochs_count</th>\n",
       "      <th>η</th>\n",
       "      <th>seed</th>\n",
       "      <th>wall_time</th>\n",
       "      <th>message_count (sum)</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>centralized</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>42</td>\n",
       "      <td>6.788240</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>centralized</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>42</td>\n",
       "      <td>6.122064</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>centralized</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>42</td>\n",
       "      <td>6.140187</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>centralized</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>42</td>\n",
       "      <td>6.120686</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>centralized</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>42</td>\n",
       "      <td>6.140917</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round    algorithm  clients_count  active_clients_fraction  batch_size  \\\n",
       "0      1  centralized              1                      NaN        1024   \n",
       "1      2  centralized              1                      NaN        1024   \n",
       "2      3  centralized              1                      NaN        1024   \n",
       "3      4  centralized              1                      NaN        1024   \n",
       "4      5  centralized              1                      NaN        1024   \n",
       "\n",
       "   local_epochs_count    η  seed  wall_time  message_count (sum)  \\\n",
       "0                   1  0.5    42   6.788240                   -1   \n",
       "1                   1  0.5    42   6.122064                   -1   \n",
       "2                   1  0.5    42   6.140187                   -1   \n",
       "3                   1  0.5    42   6.120686                   -1   \n",
       "4                   1  0.5    42   6.140917                   -1   \n",
       "\n",
       "   test_accuracy  \n",
       "0         0.8131  \n",
       "1         0.9645  \n",
       "2         0.9786  \n",
       "3         0.9745  \n",
       "4         0.9802  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centralized_df = result_centralized.as_df()\n",
    "centralized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the template with some setup steps common to all decentralized algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentralizedServer(AbstractServer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_subsets: list[Subset],\n",
    "        active_clients_fraction: float,\n",
    "        learning_rate: float,\n",
    "        batch_size: int,\n",
    "        seed: int,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            RoundParameters(\n",
    "                clients_count=len(client_subsets),\n",
    "                active_clients_fraction=active_clients_fraction,\n",
    "                batch_size=batch_size,\n",
    "                local_epochs_count=1,\n",
    "                learning_rate=learning_rate,\n",
    "                seed=seed,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.generator = npr.default_rng(seed)\n",
    "        self.clients_per_round = max(1, int(len(client_subsets) * active_clients_fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two federated learning algorithms from the paper follow, alongside an overview of metric plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the FedSGD algorithm, the baseline from the paper, we first need to define the client, and we choose to pass gradients from the client as the update result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClient(AbstractClient):\n",
    "    def __init__(self, client_data: Subset) -> None:\n",
    "        client_data_size = len(client_data)\n",
    "\n",
    "        super().__init__(client_data, client_data_size)\n",
    "        self.client_data_size = client_data_size\n",
    "\n",
    "    def update(self, weights: list[torch.Tensor], seed: int) -> list[torch.Tensor]:\n",
    "        self.generator.manual_seed(seed)\n",
    "        model = self.build_local_model(weights)\n",
    "        \n",
    "        model.train()\n",
    "        for batch_features, batch_target in self.loader_train:\n",
    "            batch_features = typing.cast(torch.Tensor, batch_features).to(device)\n",
    "            batch_target = typing.cast(torch.Tensor, batch_target).to(device)\n",
    "\n",
    "            batch_output = model(batch_features)\n",
    "\n",
    "            batch_loss = loss_function(batch_output, batch_target)\n",
    "            batch_loss.backward()\n",
    "\n",
    "        parameter_gradients = [typing.cast(torch.Tensor, param.grad).detach().cpu() for param in model.parameters()]\n",
    "        return parameter_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the corresponding server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedSgdGradientServer(DecentralizedServer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_subsets: list[Subset],\n",
    "        active_clients_fraction: float,\n",
    "        learning_rate: float,\n",
    "        seed: int,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            client_subsets=client_subsets,\n",
    "            active_clients_fraction=active_clients_fraction,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=-1,\n",
    "            seed=seed,\n",
    "        )\n",
    "        self.optimizer = SGD(params=self.model.parameters(), lr=learning_rate)\n",
    "        self.clients: list[GradientClient] = [\n",
    "            GradientClient(subset) for subset in client_subsets\n",
    "        ]\n",
    "        self.client_datasets = client_subsets\n",
    "\n",
    "    def select_clients(self) -> IndexVector:\n",
    "        return self.generator.choice(len(self.clients), self.clients_per_round)\n",
    "\n",
    "    def calculate_gradient_fraction_for_client(\n",
    "        self,\n",
    "        client: GradientClient,\n",
    "        weights: list[torch.Tensor],\n",
    "        seed: int,\n",
    "        total_epoch_dataset_size: int,\n",
    "    ) -> list[torch.Tensor]:\n",
    "        client_dataset_size = client.client_data_size\n",
    "        print(\n",
    "            f\"running training on client with training dataset of size {client_dataset_size}\"\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            client_dataset_size / total_epoch_dataset_size * gradient_component\n",
    "            for gradient_component in client.update(weights, seed)\n",
    "        ]\n",
    "\n",
    "    def run_epoch(self, weights: list[torch.Tensor], epoch: int):\n",
    "        client_indices = self.select_clients()\n",
    "        client_dataset_size = sum(\n",
    "            len(self.clients[client_idx].loader_train) for client_idx in client_indices\n",
    "        )\n",
    "\n",
    "        # N x M; N clients with gradients for M parameters each\n",
    "        gradients = [\n",
    "            self.calculate_gradient_fraction_for_client(\n",
    "                self.clients[client_idx],\n",
    "                weights,\n",
    "                seed=self.parameters.seed + epoch + 1,\n",
    "                total_epoch_dataset_size=client_dataset_size,\n",
    "            )\n",
    "            for client_idx in client_indices\n",
    "        ]\n",
    "\n",
    "        aggregated_gradient: list[torch.Tensor] = [\n",
    "            # sum gradients parameter-wise; 'parameter_gradients' is a tuple that contains one gradient per client\n",
    "            torch.stack(parameter_gradients, dim=0).sum(dim=0)\n",
    "            for parameter_gradients in zip(*gradients)\n",
    "        ]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for parameter, parameter_gradient in zip(\n",
    "                self.model.parameters(), aggregated_gradient\n",
    "            ):\n",
    "                parameter.grad = parameter_gradient.to(device)\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def run(self, rounds: int) -> RunResult:\n",
    "        metrics = RunMetrics()\n",
    "\n",
    "        for epoch in tqdm(range(rounds), \"epoch\", leave=False):\n",
    "            weights = [\n",
    "                parameter.detach().clone() for parameter in self.model.parameters()\n",
    "            ]\n",
    "\n",
    "            wall_clock_start = time.perf_counter()\n",
    "            weights = self.run_epoch(weights, epoch)\n",
    "            wall_clock_end = time.perf_counter()\n",
    "\n",
    "            accuracy = self.evaluate_accuracy()\n",
    "            execution_time_s = wall_clock_end - wall_clock_start\n",
    "\n",
    "            metrics.test_accuracy.append(accuracy)\n",
    "            metrics.wall_time.append(execution_time_s)\n",
    "            metrics.message_count.append(2 * self.clients_per_round * (epoch + 1))\n",
    "\n",
    "        return RunResult(\"FedSgd\", self.parameters, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 1/5 [00:02<00:11,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  1435 total:  10000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 2/5 [00:06<00:10,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  1032 total:  10000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 3/5 [00:10<00:06,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  980 total:  10000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 4/5 [00:13<00:03,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  980 total:  10000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n",
      "running training on client with training dataset of size 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m client_datasets \u001b[38;5;241m=\u001b[39m partition_dataset(\n\u001b[1;32m      2\u001b[0m     train_dataset,\n\u001b[1;32m      3\u001b[0m     index_by_approximate_binary_target_partitions(\n\u001b[1;32m      4\u001b[0m         partitions_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, generator_or_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      5\u001b[0m     ),\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m fedsgd_gradient_server \u001b[38;5;241m=\u001b[39m FedSgdGradientServer(\n\u001b[1;32m      9\u001b[0m     client_subsets\u001b[38;5;241m=\u001b[39mclient_datasets,\n\u001b[1;32m     10\u001b[0m     active_clients_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     11\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m,\n\u001b[1;32m     12\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m result_fedsgd_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mfedsgd_gradient_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m fedsgd_gradient_df \u001b[38;5;241m=\u001b[39m result_fedsgd_gradient\u001b[38;5;241m.\u001b[39mas_df()\n\u001b[1;32m     16\u001b[0m fedsgd_gradient_df\n",
      "Cell \u001b[0;32mIn[17], line 83\u001b[0m, in \u001b[0;36mFedSgdGradientServer.run\u001b[0;34m(self, rounds)\u001b[0m\n\u001b[1;32m     78\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m     parameter\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m     80\u001b[0m ]\n\u001b[1;32m     82\u001b[0m wall_clock_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 83\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m wall_clock_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     86\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_accuracy()\n",
      "Cell \u001b[0;32mIn[17], line 50\u001b[0m, in \u001b[0;36mFedSgdGradientServer.run_epoch\u001b[0;34m(self, weights, epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m client_dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients[client_idx]\u001b[38;5;241m.\u001b[39mloader_train) \u001b[38;5;28;01mfor\u001b[39;00m client_idx \u001b[38;5;129;01min\u001b[39;00m client_indices\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# N x M; N clients with gradients for M parameters each\u001b[39;00m\n\u001b[1;32m     49\u001b[0m gradients \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_gradient_fraction_for_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_epoch_dataset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_dataset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m client_idx \u001b[38;5;129;01min\u001b[39;00m client_indices\n\u001b[1;32m     57\u001b[0m ]\n\u001b[1;32m     59\u001b[0m aggregated_gradient: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# sum gradients parameter-wise; 'parameter_gradients' is a tuple that contains one gradient per client\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack(parameter_gradients, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m parameter_gradients \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgradients)\n\u001b[1;32m     63\u001b[0m ]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mFedSgdGradientServer.calculate_gradient_fraction_for_client\u001b[0;34m(self, client, weights, seed, total_epoch_dataset_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m client_dataset_size \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mclient_data_size\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning training on client with training dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_dataset_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     38\u001b[0m     client_dataset_size \u001b[38;5;241m/\u001b[39m total_epoch_dataset_size \u001b[38;5;241m*\u001b[39m gradient_component\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m gradient_component \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mGradientClient.update\u001b[0;34m(self, weights, seed)\u001b[0m\n\u001b[1;32m     19\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss_function(batch_output, batch_target)\n\u001b[1;32m     20\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m parameter_gradients \u001b[38;5;241m=\u001b[39m [\u001b[43mtyping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parameter_gradients\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client_datasets = partition_dataset(\n",
    "    train_dataset,\n",
    "    index_by_approximate_binary_target_partitions(\n",
    "        partitions_count=20, generator_or_seed=42\n",
    "    ),\n",
    ")\n",
    "\n",
    "fedsgd_gradient_server = FedSgdGradientServer(\n",
    "    client_subsets=client_datasets,\n",
    "    active_clients_fraction=0.2,\n",
    "    learning_rate=0.02,\n",
    "    seed=42,\n",
    ")\n",
    "result_fedsgd_gradient = fedsgd_gradient_server.run(5)\n",
    "fedsgd_gradient_df = result_fedsgd_gradient.as_df()\n",
    "fedsgd_gradient_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FedAvg algorithm is the paper's main contribution, requiring a client that passes around weights instead of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightClient(AbstractClient):\n",
    "    def __init__(\n",
    "        self, client_data: Subset, lr: float, batch_size: int, nr_epochs: int\n",
    "    ) -> None:\n",
    "        super().__init__(client_data, batch_size)\n",
    "        self.optimizer = SGD(params=self.model.parameters(), lr=lr)\n",
    "        self.nr_epochs = nr_epochs\n",
    "\n",
    "    def update(self, weights: list[torch.Tensor], seed: int) -> list[torch.Tensor]:\n",
    "        # build new model and configure the server's weights\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we define the actual server code for the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvgServer(DecentralizedServer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        batch_size: int,\n",
    "        client_subsets: list[Subset],\n",
    "        client_fraction: float,\n",
    "        nr_local_epochs: int,\n",
    "        seed: int,\n",
    "    ) -> None:\n",
    "        super().__init__(client_subsets, client_fraction, lr, batch_size, seed)\n",
    "        self.local_epochs_count = nr_local_epochs\n",
    "        self.clients = [\n",
    "            WeightClient(subset, lr, batch_size, nr_local_epochs)\n",
    "            for subset in client_subsets\n",
    "        ]\n",
    "\n",
    "    def run(self, rounds: int) -> RunResult:\n",
    "        metrics = RunMetrics()\n",
    "\n",
    "    \n",
    "\n",
    "        return RunResult(\n",
    "            \"FedAvg\",\n",
    "            self.parameters,\n",
    "            RunMetrics()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fedavg_server \u001b[38;5;241m=\u001b[39m FedAvgServer(\u001b[38;5;241m0.02\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[43msample_split\u001b[49m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m result_fedavg \u001b[38;5;241m=\u001b[39m fedavg_server\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m fedavg_df \u001b[38;5;241m=\u001b[39m result_fedavg\u001b[38;5;241m.\u001b[39mas_df()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_split' is not defined"
     ]
    }
   ],
   "source": [
    "fedavg_server = FedAvgServer(0.02, 200, sample_split, 0.2, 2, 42)\n",
    "result_fedavg = fedavg_server.run(5)\n",
    "fedavg_df = result_fedavg.as_df()\n",
    "fedavg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at a quick example of plotting the accuracy per round of the two algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.concat([fedavg_df, fedsgd_gradient_df], ignore_index=True)\n",
    "ax = sns.lineplot(df, x=\"Round\", y=\"Test accuracy\", hue=\"Algorithm\", seed=0)\n",
    "_ = ax.set_xticks(df[\"Round\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
