{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_path = \"../../datasets\"\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # normalize by mean and standard devia,\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST(data_path, train=False, download=False, transform=transform),\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    batch_size=10000,\n",
    "    generator=torch.Generator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_distinct_values(targets: typing.Iterable[typing.Any]):\n",
    "    targets_count = Counter(targets)\n",
    "\n",
    "    for target, count in targets_count.items():\n",
    "        print(f\"Target {target}: {count} records\")\n",
    "\n",
    "    print(f\"Total: {targets_count.total()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 5: 5421 records\n",
      "Target 0: 5923 records\n",
      "Target 4: 5842 records\n",
      "Target 1: 6742 records\n",
      "Target 9: 5949 records\n",
      "Target 2: 5958 records\n",
      "Target 3: 6131 records\n",
      "Target 6: 5918 records\n",
      "Target 7: 6265 records\n",
      "Target 8: 5851 records\n",
      "Total: 60000\n"
     ]
    }
   ],
   "source": [
    "count_distinct_values(train_dataset.targets.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== subset #0 ====\n",
      "Target 2: 3000 records\n",
      "Target 7: 3000 records\n",
      "Total: 6000\n",
      "==== subset #1 ====\n",
      "Target 3: 3000 records\n",
      "Target 6: 2935 records\n",
      "Target 7: 65 records\n",
      "Total: 6000\n",
      "==== subset #2 ====\n",
      "Target 1: 3665 records\n",
      "Target 2: 2335 records\n",
      "Total: 6000\n",
      "==== subset #3 ====\n",
      "Target 2: 623 records\n",
      "Target 3: 2377 records\n",
      "Target 5: 17 records\n",
      "Target 6: 2983 records\n",
      "Total: 6000\n",
      "==== subset #4 ====\n",
      "Target 3: 754 records\n",
      "Target 4: 2246 records\n",
      "Target 0: 2923 records\n",
      "Target 1: 77 records\n",
      "Total: 6000\n",
      "==== subset #5 ====\n",
      "Target 8: 51 records\n",
      "Target 9: 2949 records\n",
      "Target 0: 3000 records\n",
      "Total: 6000\n",
      "==== subset #6 ====\n",
      "Target 4: 596 records\n",
      "Target 5: 2404 records\n",
      "Target 7: 200 records\n",
      "Target 8: 2800 records\n",
      "Total: 6000\n",
      "==== subset #7 ====\n",
      "Target 5: 3000 records\n",
      "Target 4: 3000 records\n",
      "Total: 6000\n",
      "==== subset #8 ====\n",
      "Target 7: 3000 records\n",
      "Target 8: 3000 records\n",
      "Total: 6000\n",
      "==== subset #9 ====\n",
      "Target 9: 3000 records\n",
      "Target 1: 3000 records\n",
      "Total: 6000\n"
     ]
    }
   ],
   "source": [
    "from data_splitting import (\n",
    "    index_by_approximate_binary_target_partitions,\n",
    "    partition_dataset,\n",
    "    IndexVector,\n",
    ")\n",
    "\n",
    "sample_index = index_by_approximate_binary_target_partitions(train_dataset, 10, 42)\n",
    "subsets = partition_dataset(train_dataset, sample_index)\n",
    "\n",
    "for subset_idx, subset in enumerate(subsets):\n",
    "    print(f\"==== subset #{subset_idx} ====\")\n",
    "    subset_targets = list(int(it) for _, it in DataLoader(subset))\n",
    "    count_distinct_values(subset_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
